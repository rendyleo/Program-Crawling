{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwVuy24+TX+I+WgQbO1eEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rendyleo/Program-Crawling/blob/main/Crawling%20Berita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawling adalah proses otomatis untuk mengumpulkan atau menelusuri halaman-halaman web di internet menggunakan program khusus yang disebut web crawler atau spider. Web crawler secara sistematis mengikuti tautan dari satu halaman ke halaman lain, mengunduh dan menyimpan konten halaman tersebut untuk keperluan analisis, pengindeksan, atau pengumpulan data.\n",
        "\n",
        "Proses crawling biasanya dilakukan oleh mesin pencari seperti Google, Bing, atau Yahoo! untuk mengindeks halaman web sehingga pengguna bisa menemukan informasi melalui hasil pencarian. Namun, crawling juga dapat digunakan dalam banyak aplikasi lain, seperti riset pasar, analisis web, atau pengumpulan data yang terstruktur dari web.\n",
        "\n",
        " Cara Kerja Crawling:\n",
        "1. Starting Point: Crawler memulai dari satu atau beberapa URL yang disebut \"seed\" (awal penjelajahan).\n",
        "2. Menjelajahi Link: Crawler menelusuri halaman yang ditemukan dan mengumpulkan semua tautan yang ada di halaman tersebut.\n",
        "3. Mengunduh Halaman: Halaman web diunduh dan disimpan oleh crawler.\n",
        "4. Pengulangan: Crawler kemudian mengikuti tautan yang ditemukan pada halaman-halaman yang telah dikunjungi, dan proses ini terus berlanjut secara rekursif.\n",
        "\n",
        " Kegunaan Crawling:\n",
        "- Pengindeksan Mesin Pencari: Mesin pencari menggunakan crawler untuk mengindeks konten halaman web dan membuatnya tersedia dalam hasil pencarian.\n",
        "- Web Scraping: Crawling digunakan untuk mengekstrak data tertentu dari halaman web, misalnya untuk riset pasar atau pengumpulan data produk di situs e-commerce.\n",
        "- Monitoring Situs Web: Crawlers bisa digunakan untuk memantau perubahan konten pada situs web, misalnya untuk melihat perubahan harga produk atau ketersediaan informasi baru.\n",
        "  \n",
        " Contoh Crawler:\n",
        "- Googlebot: Crawler yang digunakan oleh Google untuk mengindeks halaman-halaman web dan meng-update database mesin pencari mereka.\n",
        "- Scrapy: Sebuah framework open-source yang sering digunakan untuk melakukan crawling dan scraping data dari situs web.\n",
        "\n",
        "Tantangan dalam Crawling:\n",
        "- Robots.txt: Banyak situs web yang menggunakan file `robots.txt` untuk memberikan instruksi kepada crawler tentang halaman mana yang boleh atau tidak boleh diakses.\n",
        "- Kinerja & Bandwidth: Proses crawling dapat membebani server web jika dilakukan secara berlebihan atau tidak terkendali.\n",
        "- Situs Dinamis: Banyak situs modern menggunakan JavaScript untuk memuat konten secara dinamis, yang dapat mempersulit crawling tradisional yang hanya mengunduh HTML statis.\n",
        "\n",
        "Crawling adalah bagian fundamental dari cara internet bekerja, terutama dalam hal pencarian informasi secara otomatis dan sistematis."
      ],
      "metadata": {
        "id": "PWDohihxhPWu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YbFVelDFM5_",
        "outputId": "0b5bd46c-9141-48f1-a4f2-54dac1b8fd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 lxml pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. requests\n",
        "\n",
        "Pustaka requests digunakan untuk mengirimkan permintaan HTTP ke server dan menerima respons berupa halaman web (biasanya dalam format HTML). Ini adalah langkah pertama dalam web scraping untuk mengakses konten dari situs web.\n",
        "\n",
        "Kegunaan:\n",
        "\n",
        "Mengirimkan permintaan GET atau POST ke situs web.\n",
        "Mendapatkan konten halaman web dalam format HTML atau JSON.\n",
        "\n",
        "2. BeautifulSoup\n",
        "Pustaka BeautifulSoup dari library bs4 digunakan untuk mem-parsing (mengurai) dan mengekstrak informasi dari HTML atau XML. Ini memudahkan penanganan elemen-elemen web seperti tag <div>, <a>, <table>, dll.\n",
        "\n",
        "Kegunaan:\n",
        "\n",
        "Memahami dan menavigasi struktur dokumen HTML.\n",
        "Mengambil informasi tertentu (seperti teks, tabel, gambar, link, dll.) berdasarkan elemen HTML dan atributnya."
      ],
      "metadata": {
        "id": "ygmsC2YviUDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nF5G3kPEiP_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# URL halaman utama dari situs berita (misal: halaman kategori atau halaman utama berita)\n",
        "url = 'https://sport.detik.com/sepakbola'\n",
        "\n",
        "# Header untuk menghindari pemblokiran bot\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Fungsi untuk melakukan crawling halaman berita\n",
        "def crawl_news(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Gagal mengakses halaman:\", url)\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    # Struktur halaman situs berita dapat bervariasi, pastikan sesuai dengan situs target\n",
        "    articles = soup.find_all('article')  # Atau elemen lain yang memuat berita\n",
        "    news_data = []\n",
        "\n",
        "    # Mengumpulkan data dari setiap artikel\n",
        "    for article in articles:\n",
        "        title_tag = article.find('h2', class_='title')  # Tag dan class sesuai situs yang di-crawl\n",
        "        if title_tag:\n",
        "            title = title_tag.text.strip()\n",
        "            link = article.find('a')['href']\n",
        "            date = article.find('time').text.strip()  # Sesuaikan jika berbeda\n",
        "\n",
        "            # Menambahkan ke list\n",
        "            news_data.append({\n",
        "                'title': title,\n",
        "                'url': link,\n",
        "                'date': date\n",
        "            })\n",
        "\n",
        "    return news_data\n",
        "\n",
        "# Fungsi untuk mengambil isi dari halaman detail berita\n",
        "def get_news_content(news_url):\n",
        "    response = requests.get(news_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Gagal mengakses halaman berita: {news_url}\")\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    # Temukan elemen yang berisi isi berita\n",
        "    content = soup.find('div', class_='article-body')  # Sesuaikan dengan struktur HTML situs\n",
        "    if content:\n",
        "        paragraphs = content.find_all('p')\n",
        "        news_content = \"\\n\".join([p.text.strip() for p in paragraphs])\n",
        "        return news_content\n",
        "    return \"Tidak ada konten yang ditemukan.\"\n",
        "\n",
        "# Inisialisasi list untuk menyimpan semua berita\n",
        "all_news = []\n",
        "\n",
        "# Loop untuk mengambil beberapa halaman (misal, 5 halaman, asumsikan tiap halaman memuat 10 berita)\n",
        "for i in range(1, 6):  # Loop untuk halaman 1 hingga 5\n",
        "    page_url = f'{url}?page={i}'  # Sesuaikan parameter pagination situs target\n",
        "    news_on_page = crawl_news(page_url)\n",
        "    if news_on_page:\n",
        "        for news in news_on_page:\n",
        "            news_url = news['url']\n",
        "            content = get_news_content(news_url)\n",
        "            news['content'] = content\n",
        "            all_news.append(news)\n",
        "\n",
        "# Membatasi hasil ke 50 berita\n",
        "all_news = all_news[:50]\n",
        "\n",
        "# Menyimpan hasil crawling ke dalam DataFrame\n",
        "df = pd.DataFrame(all_news)\n",
        "\n",
        "# Menampilkan data\n",
        "print(df.head())\n",
        "\n",
        "# Menyimpan data ke CSV\n",
        "df.to_csv('crawled_news_with_content.csv', index=False)\n",
        "print(\"Data disimpan ke file crawled_news_with_content.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-80CAJvnFNyX",
        "outputId": "64871159-e935-4d39-af2e-be1021f1c281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "Data disimpan ke file crawled_news_with_content.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "efY9ZJ62FlxA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}